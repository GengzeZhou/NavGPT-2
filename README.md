<div align="center">

<h1>ğŸ‡NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models</h1>

<div>
    <a href='https://gengzezhou.github.io' target='_blank'>Gengze Zhou<sup>ğŸ•</sup></a>;
    <a href='http://www.yiconghong.me' target='_blank'>Yicong Hong<sup>ğŸŒ­</sup></a>;
    <a href='https://zunwang1.github.io' target='_blank'>Zun Wang<sup>ğŸ”</sup></a>;
    <a href='https://eric-xw.github.io' target='_blank'>Xin Eric Wang<sup>ğŸŒ®</sup></a>;
    <a href='http://www.qi-wu.me' target='_blank'>Qi Wu<sup>ğŸ•</sup></a>
</div>
<sup>ğŸ•</sup>AIML, University of Adelaide 
<sup>ğŸŒ­</sup>Adobe Research 
<sup>ğŸ”</sup>Shanghai AI Laboratory 
<sup>ğŸŒ®</sup>University of California, Santa Cruz

<br>

<div>
    <a href='https://github.com/GengzeZhou/NavGPT-2' target='_blank'><img alt="Static Badge" src="https://img.shields.io/badge/NavGPT-v0.2-blue"></a>
    <a href='https://arxiv.org/abs/2305.16986' target='_blank'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
    <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a>
    <a href="https://github.com/salesforce/LAVIS"><img alt="Static Badge" src="https://img.shields.io/badge/Salesforce-LAVIS-blue?logo=salesforce"></a>
</div>

</div>


## ğŸ¹ Abstract
 Capitalizing on the remarkable advancements in Large Language Models (LLMs), there is a burgeoning initiative to harness LLMs for instruction following robotic navigation. Such a trend underscores the potential of LLMs to generalize navigational reasoning and diverse language understanding. However, a significant discrepancy in agent performance is observed when integrating LLMs in the Vision-and-Language navigation (VLN) tasks compared to previous downstream specialist models. Furthermore, the inherent capacity of language to interpret and facilitate communication in agent interactions is often underutilized in these integrations. In this work, we strive to bridge the divide between VLN-specialized models and LLM-based navigation paradigms, while maintaining the interpretative prowess of LLMs in generating linguistic navigational reasoning. By aligning visual content in a frozen LLM, we encompass visual observation comprehension for LLMs and exploit a way to incorporate LLMs and navigation policy networks for effective action predictions and navigational reasoning. We demonstrate the data efficiency of the proposed methods and eliminate the gap between LM-based agents and state-of-the-art VLN specialists.

## ğŸ¸ Method
![](assets/NavGPT-2.png)

## ğŸ» TODOs

- [ ] Release ğŸ‡NavGPT-2 policy finetuning code.
- [ ] Release visual instruction tuning code.
- [ ] Release navigational reasoning data.
- [ ] Release pretrained models weights.
